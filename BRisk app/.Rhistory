rm(list=ls())
# Reduce contamination frequency
# Define the probability vectors
prob50 <- c(0.5, 0.5)
prob10 <- c(0.1, 0.9)
result50 = rep(x, round(n * prob50))
result10 = rep(x, round(n * prob10))
# Sample from the probability vectors to create the contamination status
contam_BySample_freq50<-as.vector(replicate(n_sim, sample(result50)))
## Set simulation setting
n_sim = 10000
n_halfgal = 10
# Reduce contamination frequency
# Define the probability vectors
prob50 <- c(0.5, 0.5)
prob10 <- c(0.1, 0.9)
result50 = rep(x, round(n * prob50))
result10 = rep(x, round(n * prob10))
library(shiny); runApp('C:/Users/sujun/OneDrive/Documents/RWork/PPC gram negative bacteria in fluid milk/App/App_PPC_new.R')
# Reduce contamination frequency
# Define the probability vectors
x = c(1,0)
prob50 <- c(0.5, 0.5)
prob10 <- c(0.1, 0.9)
result50 = rep(x, round(n * prob50))
result10 = rep(x, round(n * prob10))
runApp('C:/Users/sujun/OneDrive/Documents/RWork/PPC gram negative bacteria in fluid milk/App/App_PPC_new.R')
rm(list=ls())
rm(list=ls())
rm(list=ls())
library(shiny); runApp('GitHub/Bacillus-cereus-exposure-assessment-model/BRisk app/BRisk.R')
rm(list=ls())
library(shiny); runApp('GitHub/Bacillus-cereus-exposure-assessment-model/BRisk app/BRisk.R')
rm(list=ls())
library(shiny); runApp('GitHub/Bacillus-cereus-exposure-assessment-model/BRisk app/BRisk.R')
rm(list=ls())
library(shiny); runApp('GitHub/Bacillus-cereus-exposure-assessment-model/BRisk app/BRisk.R')
rm(list=ls())
runApp('GitHub/Bacillus-cereus-exposure-assessment-model/BRisk app/BRisk.R')
rm(list=ls())
knitr::opts_chunk$set(echo = TRUE)
#install.packages("lmtest")
library("lmtest")
n <- 200
# generate covariate X
x <- runif(n, 1, 10)
# generate Y1 with homoscedasticity
sd1 <- mean(x^2)
sd1
y1 <- 1 + 2 * x + rnorm(n, sd = sd1)
# generate Y2 with heteroscedasticity
y2 <- 1 + 2 * x + rnorm(n, sd = x^2)
# fit linear model
mod1 <- lm(y1 ~ x)
mod2 <- lm(y2 ~ x)
par(mfrow = c(1,2))
plot(x, y1, main = "Model 1: Homoscedastic")
plot(x, mod1$res, main = "Model 1: Homoscedastic", ylab = "Residuals")
par(mfrow = c(1,2))
plot(x, y2, main = "Model 2: Heteroscedastic")
plot(x, mod2$res, main = "Model 1: Heteroscedastic", ylab = "Residuals")
## Standardized Squared residuals
stSqRes1 <- mod1$residuals^2 / mean(mod1$residuals^2)
stSqRes2 <- mod2$residuals^2 / mean(mod2$residuals^2)
par(mfrow = c(1,2))
plot(x, stSqRes1, ylab = "Standardized Sq Res", xlab = "Covariate")
abline(a = lm(stSqRes1~x)$coef[1], b = lm(stSqRes1~x)$coef[2], col = "red")
plot(x, stSqRes2, ylab = "Standardized Sq Res", xlab = "Covariate")
abline(a = lm(stSqRes2~x)$coef[1], b = lm(stSqRes2~x)$coef[2], col = "red")
# run breusch pagan test
# use the bptest function (in the lmtest package)
# takes the fitted linear model as an argument
bptest(mod1)
bptest(mod2)
sim.size <- 1000
b <- 0
n <- 200
# generate covariate X
rec <- matrix(0, sim.size, 4)
for(i in 1:sim.size){
x <- rgamma(n, 1, 1)
# generate Y2 with homoscedasticity
sd1 <- mean(x)
y1 <- 1 + b * x + rnorm(n, sd = sd1)
# generate Y2 with heteroscedasticity
y2 <- 1 + b * x + rnorm(n, sd = x)
mod1 <- lm(y1 ~ x)
mod2 <- lm(y2 ~ x)
# fit linear model
output1 <- summary(mod1)$coeff
output2 <- summary(mod2)$coeff
rec[i, ] <- c(output1[2,3:4], output2[2,3:4])
}
x <- seq(-5, 5, by = .01)
par(mfrow = c(1,2))
hist(rec[, 1], freq = F, main = "Homoscedastic")
lines(x, dt(x, df = n - 2), col = "red")
hist(rec[, 3], freq = F, main = "Heteroscedastic")
lines(x, dt(x, df = n - 2), col = "red")
par(mfrow = c(1,2))
hist(rec[, 2], freq = F, main = "Homoscedastic")       # constant p values
hist(rec[, 4], freq = F, main = "Heteroscedastic")     # inflation of p values
# install.packages("sandwich)
library("sandwich")
## Get model based standard errors
## vcov returns hat sigma^2 (X'X)^{-1}
## which is the estimated covariance matrix of b-hat
vcov(mod2)
# diag gets the diagonal elements of the matrix
# these elements correspond to the estimated variance of the coefficients
# We take the square root to get the standard error
sqrt(diag(vcov(mod2)))
# Check to see that this is the same as
summary(mod2)  # standard error of intercept and slope
## Get sandwich standard errors
## vcovHC (from the sandwich package) returns
# hat sigma^2 (X'X)^{-1} (X' W-hat X) (X'X)^{-1} which is
## the estimated covariance matrix of b-hat allowing for heteroscedasticity
# type = "HC3" is a specific way to estimate the W-hat matrix
# and is the most popular procedure
vcovHC(mod2, type = "HC3")
# Get the standard deviation of each of
sqrt(diag(vcovHC(mod2, type = "HC3")))
# Check to see that this is NOT the same as
summary(mod2)  # robust standard errors are different from model based standard errors
## Use the coeftest function (from the lmtest package)
# by default, the coeftest function uses the "model based" standard errors
coeftest(mod2)   # hypothesis test the model coefficients
summary(mod2)
# Create 95% confidence intervals using model based standard errors
coefci(mod2, level = .95)
# Instead of using the default model based standard errors
# we can feed a specific variance matrix
# and replace the default with the robust standard errors
coeftest(mod2, vcov. = vcovHC(mod2, type = "HC3"))
# Create 95% confidence intervals using robust standard errors
coefci(mod2, level = .95, vcov. = vcovHC(mod2, type = "HC3"))
sim.size <- 1000
b <- 0
n <- 20
# generate covariate X
rec <- matrix(0, sim.size, 4)
for(i in 1:sim.size){
x <- rgamma(n, 1, 1)
# generate Y2 with homoscedasticity
sd1 <- mean(x)
y1 <- 1 + b * x + rnorm(n, sd = sd1)
# generate Y2 with heteroscedasticity
y2 <- 1 + b * x + rnorm(n, sd = x)
# fit linear model
mod1 <- lm(y1 ~ x)
mod2 <- lm(y2 ~ x)
rec[i, ] <- c(  coeftest(mod1, vcov. = vcovHC(mod1, type = "HC3"))[2,3:4],
coeftest(mod2, vcov. = vcovHC(mod2, type = "HC3"))[2,3:4])
}
x <- seq(-5, 5, by = .01)
par(mfrow = c(1,2))
hist(rec[, 1], freq = F, main = "Homoscedastic")
lines(x, dt(x, df = n - 2), col = "red")
hist(rec[, 3], freq = F, main = "Heteroscedastic")
lines(x, dt(x, df = n - 2), col = "red")
par(mfrow = c(1,2))
hist(rec[, 2], freq = F, main = "Homoscedastic")
hist(rec[, 4], freq = F, main = "Heteroscedastic")  # more constant p-values
# When using robust standard errors, losing a little bit power, more likely to make Type II error
sim.size <- 1000
b <- 1/4
n <- 20
# generate covariate X
rec <- matrix(0, sim.size, 8)
for(i in 1:sim.size){
x <- rgamma(n, 1, 1)
# generate Y2 with homoscedasticity
sd1 <- mean(x)
y1 <- 1 + b * x + rnorm(n, sd = sd1)
# generate Y2 with heteroscedasticity
y2 <- 1 + b * x + rnorm(n, sd = x)
# fit linear model
mod1 <- lm(y1 ~ x)
mod2 <- lm(y2 ~ x)
output1 <- summary(mod1)$coeff
output2 <- summary(mod2)$coeff
rec[i, ] <- c(output1[2,3:4],
output2[2,3:4],
coeftest(mod1, vcov. = vcovHC(mod1, type = "HC3"))[2,3:4],
coeftest(mod2, vcov. = vcovHC(mod2, type = "HC3"))[2,3:4])
}
par(mfrow = c(2,2))
hist(rec[, 2], freq = F, main = "Homoscedastic (Model SE)")
hist(rec[, 4], freq = F, main = "Heteroscedastic (Model SE)")
hist(rec[, 6], freq = F, main = "Homoscedastic (Sandw SE)")
hist(rec[, 8], freq = F, main = "Heteroscedastic (Sandw SE)")
fileName <- url("https://raw.githubusercontent.com/ysamwang/btry6020_sp22/main/lectureData/estate.csv")
housing_data <- read.csv(fileName)
names(housing_data)
#fitting the model
hmod <- lm(log(price)~log(area) + log(lot) + bed + bath, data = housing_data)
#Testing for heteroscedasticity
bptest(hmod)    # there is heteroscedasticity in the data
coeftest(hmod)   # model based standard error
coeftest(hmod,vcov.=vcovHC(hmod,type = "HC3"))   # robust standard error
sim.size <- 1000
n <- 50
rec1 <- rep(0, sim.size)
for(i in 1:sim.size){
rec1[i] <- mean(rgamma(n, 1, 1))
}
hist(rec1,breaks=20)
x <- rgamma(n, 1, 1)
newX <- sample(x, replace = T)  # draw a sample with the same size as x with replacement
sim.size <- 1000
n <- 50
rec2 <- rep(0, sim.size)
x <- rgamma(n, 1, 1)
for(i in 1:sim.size){
rec2[i] <- mean(sample(x, replace = T) )
}
hist(rec2,breaks=20)
par(mfrow=c(1,2))
hist(rec1,breaks = 20)
hist(rec2,breaks = 20)
rm(list=ls())
knitr::opts_chunk$set(echo = TRUE)
## Homoscedastic linear model
# Fixed Covariates
# Using Wild Bootstrap
sim.size <- 5000
b <- 1
n <- 400
# Fixed Design
x <- seq(0, 10, by = 10/(n-1))
y <- 1 + b * x + rnorm(n)
## We are interested in the estimated coefficient
mod <- lm(y~ x)
observed.stat <- summary(mod)$coef[2, 1]
### Approximate sampling distribution using the Wild and empirical bootstrap
rec.boot <- matrix(0, sim.size, 2)
for(i in 1:sim.size){
# Wild Bootstrap
y.boot.wild <- mod$fitted + mod$residuals * rnorm(n)
# Calculate the statistic for the bootstrap sample
rec.boot[i, 1] <- summary(lm(y.boot.wild ~ x))$coeff[2,1] - observed.stat
# Pairs Bootstrap
ind <- sample(n, replace = T)
x.boot.emp <- x[ind]
y.boot.emp <- y[ind]
# Calculate the statistic for the bootstrap sample
rec.boot[i, 2] <- summary(lm(y.boot.emp ~ x.boot.emp))$coeff[2,1] - observed.stat
}
rec.sim <- rep(0, sim.size)
for(i in 1:sim.size){
## Don't re-draw X
x.sim <- x
y.sim <- 1 + b * x.sim + rnorm(n)
rec.sim[i] <- summary(lm(y.sim ~ x.sim))$coeff[2,1] - b
}
par(mfrow = c(1, 2))
### Bootstrapped sampling distribution in gray histogram
hist(rec.boot[,1], freq = F, col = "gray", breaks = 15, main = "Wild", xlab = expression(hat(b)))
### Simulated Sampling Distribution in red
lines(density(rec.sim), col = "red", lwd = 2)
### Model Based Sampling Distribution in Blue
lines(seq(-5, 5, by = .01), dnorm(seq(-5, 5, by = .01), mean = 0, sd = sqrt(1 / sum((x - mean(x))^2))), col = "blue", lwd = 2)
### Bootstrapped sampling distribution in gray histogram
hist(rec.boot[,2], freq = F, col = "gray", breaks = 15, main = "Pairs", xlab =  expression(hat(b)))
### Simulated Sampling Distribution in red
lines(density(rec.sim), col = "red", lwd = 2)
### Model Based Sampling Distribution in Blue
lines(seq(-5, 5, by = .01), dnorm(seq(-5, 5, by = .01), mean = 0, sd = sqrt(1 / sum((x - mean(x))^2))), col = "blue", lwd = 2)
## Homoscedastic linear model
# Fixed Covariates
# Using Wild Bootstrap
sim.size <- 5000
b <- 1
n <- 2000
# Fixed Design
x <- seq(0, 10, by = 10/(n-1))
y <- 1 + b * x + rnorm(n, sd = x / 3)   # error gets bigger as x gets bigger, heterocedasticity
## We are interested in the estimated coefficient
mod <- lm(y~ x)
observed.stat <- summary(mod)$coef[2, 1]
### Approximate sampling distribution using the Wild and empirical bootstrap
rec.boot <- matrix(0, sim.size, 2)
for(i in 1:sim.size){
# Wild Bootstrap
y.boot.wild <- mod$fitted + mod$residuals * rnorm(n)
# Calculate the statistic for the bootstrap sample
rec.boot[i, 1] <- summary(lm(y.boot.wild ~ x))$coeff[2,1] - observed.stat
# Pairs Bootstrap
ind <- sample(n, replace = T)
x.boot.emp <- x[ind]
y.boot.emp <- y[ind]
# Calculate the statistic for the bootstrap sample
rec.boot[i, 2] <- summary(lm(y.boot.emp ~ x.boot.emp))$coeff[2,1] - observed.stat
}
rec.sim <- rep(0, sim.size)
for(i in 1:sim.size){
# don't redraw X
x.sim <- x
y.sim <- 1 + b * x.sim + rnorm(n, sd = x / 3)
rec.sim[i] <- summary(lm(y.sim ~ x.sim))$coeff[2,1] - b
}
par(mfrow = c(1, 2))
### Bootstrapped sampling distribution in gray histogram
hist(rec.boot[,1], freq = F, col = "gray", breaks = 15, main = "Wild", xlab = expression(hat(b)))
### Simulated Sampling Distribution in red
lines(density(rec.sim), col = "red", lwd = 2)
### Model Based Sampling Distribution in Blue
lines(seq(-5, 5, by = .01), dnorm(seq(-5, 5, by = .01), mean = 0, sd = sqrt(1 / sum((x - mean(x))^2))), col = "blue", lwd = 2)
### Bootstrapped sampling distribution in gray histogram
hist(rec.boot[,2], freq = F, col = "gray", breaks = 15, main = "Pairs", xlab =  expression(hat(b)))
### Simulated Sampling Distribution in red
lines(density(rec.sim), col = "red", lwd = 2)
### Model Based Sampling Distribution in Blue
lines(seq(-5, 5, by = .01), dnorm(seq(-5, 5, by = .01), mean = 0, sd = sqrt(1 / sum((x - mean(x))^2))), col = "blue", lwd = 2)
# install.packages("lmboot")
library("lmboot")
library("lmtest")
library("sandwich")
fileName <- "https://raw.githubusercontent.com/ysamwang/btry6020_sp22/main/lectureData/estate.csv"
housing_data <- read.csv(fileName)
names(housing_data)
mod <- lm(log(price) ~ log(area) + bed + bath + garage + quality,
data = housing_data)
summary(mod)
library(lme4)
fileName <- "https://raw.githubusercontent.com/ysamwang/btry6020_sp22/main/lectureData/grace_plot_level.csv"
dat <- read.csv(fileName)
# Remove Missing Data
# Generally, we want to be careful about the data we remove
# As this may bias our estimates if the missingness is important
dat <- na.omit(dat)
dim(dat)
# Fit a linear model which disregards the site structure
mod <- lm(ln.prich~ ln.ptotmass + ln.pprod, data = dat)
summary(mod)
dat
plot(mod)
rm(list = ls())
knitr::opts_chunk$set(echo = TRUE)
# hint: when feeding in the covariates, you will need to remove the first two columns
# from the bike_data_training set which correspond to the date and bike_counts
# so you will need to feed the function
x = as.matrix(bike_data_train[, - c(1,2)])
bike_data_train <- read.csv("https://raw.githubusercontent.com/ysamwang/btry6020_sp22/main/lectureData/bike_data_18.csv")
dim(bike_data_train)
names(bike_data_train)
# hint: when feeding in the covariates, you will need to remove the first two columns
# from the bike_data_training set which correspond to the date and bike_counts
# so you will need to feed the function
x = as.matrix(bike_data_train[, - c(1,2)])
library(glmnet)
cv_lasso_fit <- cv.glmnet(x = x, y = bike_data_train$bike_counts, alpha = 1, family = "poisson")
plot(cv_lasso_fit)
coef(cv_lasso_fit, s = cv_lasso_fit$lambda.min)
# hint: when feeding in the covariates, you will need to remove the first two columns
# from the bike_data_training set which correspond to the date and bike_counts
# so you will need to feed the function
x = as.matrix(bike_data_train[, - c(1,2)])
library(glmnet)
cv_lasso_fit <- cv.glmnet(x = x, y = bike_data_train$bike_counts, alpha = 1, family = "poisson")
plot(cv_lasso_fit)
coef(cv_lasso_fit, s = cv_lasso_fit$lambda.min)
coef(cv_lasso_fit, s = lasso_mod$lambda.1se)
# hint: when feeding in the covariates, you will need to remove the first two columns
# from the bike_data_training set which correspond to the date and bike_counts
# so you will need to feed the function
x = as.matrix(bike_data_train[, - c(1,2)])
library(glmnet)
cv_lasso_fit <- cv.glmnet(x = x, y = bike_data_train$bike_counts, alpha = 1, family = "poisson")
plot(cv_lasso_fit)
coef(cv_lasso_fit, s = cv_lasso_fit$lambda.min)
coef(cv_lasso_fit, s = lasso_fit$lambda.1se)
# hint: when feeding in the covariates, you will need to remove the first two columns
# from the bike_data_training set which correspond to the date and bike_counts
# so you will need to feed the function
x = as.matrix(bike_data_train[, - c(1,2)])
library(glmnet)
cv_lasso_fit <- cv.glmnet(x = x, y = bike_data_train$bike_counts, alpha = 1, family = "poisson")
plot(cv_lasso_fit)
coef(cv_lasso_fit, s = cv_lasso_fit$lambda.min)
coef(cv_lasso_fit, s = cv_lasso_fit$lambda.1se)
cv_ridge_fit <- cv.glmnet(x = x, y = bike_data_train$bike_counts, alpha = 0, family = "posson")
cv_ridge_fit <- cv.glmnet(x = x, y = bike_data_train$bike_counts, alpha = 0, family = "poisson")
plot(cv_ridge_fit)
coef(cv_ridge_fit, s = cv_ridge_fit$lambda.min)
coef(cv_ridge_fit, s = cv_ridge_fit$lambda.1se)
# Test data from 2019
bike_data_test <- read.csv("https://raw.githubusercontent.com/ysamwang/btry6020_sp22/main/lectureData/bike_data_19.csv")
# regular glm without any penalization
unpenalized_model <- glm(bike_counts ~ ., data = bike_data_train[, -1], family = "poisson")
# predictive accuracy for 2019 when using all covariates but no model seletion or penalization
## use type = "response" to get predictions in bikes, instead of log(bikes)
mean((bike_data_test$bike_counts - predict(unpenalized_model, newx = as.matrix(bike_data_test[,-c(1)]), type = "response"))^2)
## to get predictions for a penalized regression
## use type = "response" to get predictions in bikes, instead of log(bikes)
lasso_predicted <- predict(cv_lasso_fit, s = cv_lasso_fit$lambda.min, newx = x, type = "response")
lasso_test_error <- mean((bike_data_train$bike_counts - lasso_predicted)^2)
lasso_test_error
ridge_predicted <- predict(cv_ridge_fit, s = cv_ridge_fit$lambda.min, newx = x, type = "response")
ridge_test_error <- mean((bike_data_train$bike_counts - ridge_predicted)^2)
ridge_test_error
# hint: when feeding in the covariates, you will need to remove the first two columns
# from the bike_data_training set which correspond to the date and bike_counts
# so you will need to feed the function
x = as.matrix(bike_data_train[, - c(1,2)])
library(glmnet)
cv_lasso_fit <- cv.glmnet(x = x, y = bike_data_train$bike_counts, alpha = 1, family = "poisson")
plot(cv_lasso_fit)
coef(cv_lasso_fit, s = cv_lasso_fit$lambda.min)
coef(cv_lasso_fit, s = cv_lasso_fit$lambda.1se)
cv_ridge_fit <- cv.glmnet(x = x, y = bike_data_train$bike_counts, alpha = 0, family = "poisson")
plot(cv_ridge_fit)
coef(cv_ridge_fit, s = cv_ridge_fit$lambda.min)
coef(cv_ridge_fit, s = cv_ridge_fit$lambda.1se)
# Test data from 2019
bike_data_test <- read.csv("https://raw.githubusercontent.com/ysamwang/btry6020_sp22/main/lectureData/bike_data_19.csv")
# regular glm without any penalization
unpenalized_model <- glm(bike_counts ~ ., data = bike_data_train[, -1], family = "poisson")
# predictive accuracy for 2019 when using all covariates but no model seletion or penalization
## use type = "response" to get predictions in bikes, instead of log(bikes)
mean((bike_data_test$bike_counts - predict(unpenalized_model, newx = as.matrix(bike_data_test[,-c(1)]), type = "response"))^2)
## to get predictions for a penalized regression
## use type = "response" to get predictions in bikes, instead of log(bikes)
lasso_predicted <- predict(cv_lasso_fit, s = cv_lasso_fit$lambda.min, newx = x, type = "response")
lasso_test_error <- mean((bike_data_train$bike_counts - lasso_predicted)^2)
lasso_test_error
ridge_predicted <- predict(cv_ridge_fit, s = cv_ridge_fit$lambda.min, newx = x, type = "response")
ridge_test_error <- mean((bike_data_train$bike_counts - ridge_predicted)^2)
ridge_test_error
rm(list=ls())
library(shiny); runApp('GitHub/Bacillus-cereus-exposure-assessment-model/BRisk app/BRisk.R')
ModelData
runApp('GitHub/Bacillus-cereus-exposure-assessment-model/BRisk app/BRisk.R')
str(df2)
runApp('GitHub/Bacillus-cereus-exposure-assessment-model/BRisk app/BRisk.R')
rm(list=ls())
library(shiny); runApp('GitHub/Bacillus-cereus-exposure-assessment-model/BRisk app/BRisk.R')
rm(list=ls())
rm(list=ls())
?density
# Cytotoxicity data
cytotoxicity_input = read.csv("FINAL B_cereus_mastersheet.csv")
setwd("C:/Users/sujun/Documents/GitHub/Bacillus-cereus-exposure-assessment-model/BRisk app")
# Cytotoxicity data
cytotoxicity_input = read.csv("FINAL B_cereus_mastersheet.csv")
colnames(cytotoxicity_input)[1] <- "Isolate.Name"
cytotoxicity_input <- cytotoxicity_input %>%
separate(Btyper3.Closest.Type.Strain, into = c("species","ANI"), sep = "\\(") %>%
separate(Adjusted.panC.Group..predicted.species., into = c("panC_Group","predicted_species"), sep = "\\(") %>%
mutate(ANI = gsub("\\)", "", ANI),
panC_Group = gsub("\\)", "", panC_Group),
predicted_species = gsub("\\)", "", predicted_species))
library(tidyverse)
library(ggplot2)
# Cytotoxicity data
cytotoxicity_input = read.csv("FINAL B_cereus_mastersheet.csv")
colnames(cytotoxicity_input)[1] <- "Isolate.Name"
cytotoxicity_input <- cytotoxicity_input %>%
separate(Btyper3.Closest.Type.Strain, into = c("species","ANI"), sep = "\\(") %>%
separate(Adjusted.panC.Group..predicted.species., into = c("panC_Group","predicted_species"), sep = "\\(") %>%
mutate(ANI = gsub("\\)", "", ANI),
panC_Group = gsub("\\)", "", panC_Group),
predicted_species = gsub("\\)", "", predicted_species))
cytotoxicity_input
df3 = cytotoxicity_input
df2 = read.csv("Iso402.csv")
df2 <- df2 %>%
separate(Closest_Type_Strain.ANI., into = c("species","ANI"), sep = "\\(") %>%
separate(Adjusted_panC_Group.predicted_species., into = c("panC_Group","predicted_species"), sep = "\\(") %>%
mutate(ANI = gsub("\\)", "", ANI),
panC_Group = gsub("\\)", "", panC_Group),
predicted_species = gsub("\\)", "", predicted_species))
colnames(df3)[colnames(df3) == "Average.Cell.Viability"] <- "Normalized_Cytotoxicity"
matching_species_df_ct <- subset(df3, species == df2$species)
matching_species_df_ct1 <- subset(df3, panC_Group == df2$panC_Group)
# Calculate density values
density_full <- density(df3)
# Calculate density values
density_full <- density(df3$Normalized_Cytotoxicity)
density_full
density_df_ct <- density(matching_species_df_ct$Normalized_Cytotoxicity)
density_df_ct1 <- density(matching_species_df_ct1$Normalized_Cytotoxicity)
# Normalize density values against full dataset
density_df_ct$y <- density_df_ct$y / sum(density_full$y)
density_df_ct1$y <- density_df_ct1$y / sum(density_full$y)
# Create a new data frame
df <- data.frame(
x = c(density_full$x, density_df_ct$x, density_df_ct1$x),
y = c(density_full$y, density_df_ct$y, density_df_ct1$y),
group = c(rep("All isolates", length(density_full$x)),
rep("Closest type strain", length(density_df_ct$x)),
rep("Phylogenetic group", length(density_df_ct1$x)))
)
View(df)
# Create the density plot
ggplot(df, aes(x = x, y = y, fill = group)) +
geom_density(alpha = 0.5) +
labs(x = "X-axis", y = "Density", fill = "Group") +
theme_minimal()
df$y
# Create the density plot
ggplot(df, aes(x = x, y = y, fill = group)) +
geom_density(alpha = 0.5) +
labs(x = "X-axis", y = "Density", fill = "Group") +
theme_minimal()
library(shiny); runApp('BRisk.R')
runApp('BRisk.R')
runApp('BRisk.R')
runApp('BRisk.R')
runApp('BRisk.R')
library(shiny); runApp('BRisk.R')
