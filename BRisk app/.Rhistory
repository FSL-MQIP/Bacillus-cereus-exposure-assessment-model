x = c(1,0)
prob50 <- c(0.5, 0.5)
prob10 <- c(0.1, 0.9)
result50 = rep(x, round(n * prob50))
result10 = rep(x, round(n * prob10))
runApp('C:/Users/sujun/OneDrive/Documents/RWork/PPC gram negative bacteria in fluid milk/App/App_PPC_new.R')
rm(list=ls())
rm(list=ls())
rm(list=ls())
library(shiny); runApp('GitHub/Bacillus-cereus-exposure-assessment-model/BRisk app/BRisk.R')
rm(list=ls())
library(shiny); runApp('GitHub/Bacillus-cereus-exposure-assessment-model/BRisk app/BRisk.R')
rm(list=ls())
library(shiny); runApp('GitHub/Bacillus-cereus-exposure-assessment-model/BRisk app/BRisk.R')
rm(list=ls())
library(shiny); runApp('GitHub/Bacillus-cereus-exposure-assessment-model/BRisk app/BRisk.R')
rm(list=ls())
runApp('GitHub/Bacillus-cereus-exposure-assessment-model/BRisk app/BRisk.R')
rm(list=ls())
knitr::opts_chunk$set(echo = TRUE)
#install.packages("lmtest")
library("lmtest")
n <- 200
# generate covariate X
x <- runif(n, 1, 10)
# generate Y1 with homoscedasticity
sd1 <- mean(x^2)
sd1
y1 <- 1 + 2 * x + rnorm(n, sd = sd1)
# generate Y2 with heteroscedasticity
y2 <- 1 + 2 * x + rnorm(n, sd = x^2)
# fit linear model
mod1 <- lm(y1 ~ x)
mod2 <- lm(y2 ~ x)
par(mfrow = c(1,2))
plot(x, y1, main = "Model 1: Homoscedastic")
plot(x, mod1$res, main = "Model 1: Homoscedastic", ylab = "Residuals")
par(mfrow = c(1,2))
plot(x, y2, main = "Model 2: Heteroscedastic")
plot(x, mod2$res, main = "Model 1: Heteroscedastic", ylab = "Residuals")
## Standardized Squared residuals
stSqRes1 <- mod1$residuals^2 / mean(mod1$residuals^2)
stSqRes2 <- mod2$residuals^2 / mean(mod2$residuals^2)
par(mfrow = c(1,2))
plot(x, stSqRes1, ylab = "Standardized Sq Res", xlab = "Covariate")
abline(a = lm(stSqRes1~x)$coef[1], b = lm(stSqRes1~x)$coef[2], col = "red")
plot(x, stSqRes2, ylab = "Standardized Sq Res", xlab = "Covariate")
abline(a = lm(stSqRes2~x)$coef[1], b = lm(stSqRes2~x)$coef[2], col = "red")
# run breusch pagan test
# use the bptest function (in the lmtest package)
# takes the fitted linear model as an argument
bptest(mod1)
bptest(mod2)
sim.size <- 1000
b <- 0
n <- 200
# generate covariate X
rec <- matrix(0, sim.size, 4)
for(i in 1:sim.size){
x <- rgamma(n, 1, 1)
# generate Y2 with homoscedasticity
sd1 <- mean(x)
y1 <- 1 + b * x + rnorm(n, sd = sd1)
# generate Y2 with heteroscedasticity
y2 <- 1 + b * x + rnorm(n, sd = x)
mod1 <- lm(y1 ~ x)
mod2 <- lm(y2 ~ x)
# fit linear model
output1 <- summary(mod1)$coeff
output2 <- summary(mod2)$coeff
rec[i, ] <- c(output1[2,3:4], output2[2,3:4])
}
x <- seq(-5, 5, by = .01)
par(mfrow = c(1,2))
hist(rec[, 1], freq = F, main = "Homoscedastic")
lines(x, dt(x, df = n - 2), col = "red")
hist(rec[, 3], freq = F, main = "Heteroscedastic")
lines(x, dt(x, df = n - 2), col = "red")
par(mfrow = c(1,2))
hist(rec[, 2], freq = F, main = "Homoscedastic")       # constant p values
hist(rec[, 4], freq = F, main = "Heteroscedastic")     # inflation of p values
# install.packages("sandwich)
library("sandwich")
## Get model based standard errors
## vcov returns hat sigma^2 (X'X)^{-1}
## which is the estimated covariance matrix of b-hat
vcov(mod2)
# diag gets the diagonal elements of the matrix
# these elements correspond to the estimated variance of the coefficients
# We take the square root to get the standard error
sqrt(diag(vcov(mod2)))
# Check to see that this is the same as
summary(mod2)  # standard error of intercept and slope
## Get sandwich standard errors
## vcovHC (from the sandwich package) returns
# hat sigma^2 (X'X)^{-1} (X' W-hat X) (X'X)^{-1} which is
## the estimated covariance matrix of b-hat allowing for heteroscedasticity
# type = "HC3" is a specific way to estimate the W-hat matrix
# and is the most popular procedure
vcovHC(mod2, type = "HC3")
# Get the standard deviation of each of
sqrt(diag(vcovHC(mod2, type = "HC3")))
# Check to see that this is NOT the same as
summary(mod2)  # robust standard errors are different from model based standard errors
## Use the coeftest function (from the lmtest package)
# by default, the coeftest function uses the "model based" standard errors
coeftest(mod2)   # hypothesis test the model coefficients
summary(mod2)
# Create 95% confidence intervals using model based standard errors
coefci(mod2, level = .95)
# Instead of using the default model based standard errors
# we can feed a specific variance matrix
# and replace the default with the robust standard errors
coeftest(mod2, vcov. = vcovHC(mod2, type = "HC3"))
# Create 95% confidence intervals using robust standard errors
coefci(mod2, level = .95, vcov. = vcovHC(mod2, type = "HC3"))
sim.size <- 1000
b <- 0
n <- 20
# generate covariate X
rec <- matrix(0, sim.size, 4)
for(i in 1:sim.size){
x <- rgamma(n, 1, 1)
# generate Y2 with homoscedasticity
sd1 <- mean(x)
y1 <- 1 + b * x + rnorm(n, sd = sd1)
# generate Y2 with heteroscedasticity
y2 <- 1 + b * x + rnorm(n, sd = x)
# fit linear model
mod1 <- lm(y1 ~ x)
mod2 <- lm(y2 ~ x)
rec[i, ] <- c(  coeftest(mod1, vcov. = vcovHC(mod1, type = "HC3"))[2,3:4],
coeftest(mod2, vcov. = vcovHC(mod2, type = "HC3"))[2,3:4])
}
x <- seq(-5, 5, by = .01)
par(mfrow = c(1,2))
hist(rec[, 1], freq = F, main = "Homoscedastic")
lines(x, dt(x, df = n - 2), col = "red")
hist(rec[, 3], freq = F, main = "Heteroscedastic")
lines(x, dt(x, df = n - 2), col = "red")
par(mfrow = c(1,2))
hist(rec[, 2], freq = F, main = "Homoscedastic")
hist(rec[, 4], freq = F, main = "Heteroscedastic")  # more constant p-values
# When using robust standard errors, losing a little bit power, more likely to make Type II error
sim.size <- 1000
b <- 1/4
n <- 20
# generate covariate X
rec <- matrix(0, sim.size, 8)
for(i in 1:sim.size){
x <- rgamma(n, 1, 1)
# generate Y2 with homoscedasticity
sd1 <- mean(x)
y1 <- 1 + b * x + rnorm(n, sd = sd1)
# generate Y2 with heteroscedasticity
y2 <- 1 + b * x + rnorm(n, sd = x)
# fit linear model
mod1 <- lm(y1 ~ x)
mod2 <- lm(y2 ~ x)
output1 <- summary(mod1)$coeff
output2 <- summary(mod2)$coeff
rec[i, ] <- c(output1[2,3:4],
output2[2,3:4],
coeftest(mod1, vcov. = vcovHC(mod1, type = "HC3"))[2,3:4],
coeftest(mod2, vcov. = vcovHC(mod2, type = "HC3"))[2,3:4])
}
par(mfrow = c(2,2))
hist(rec[, 2], freq = F, main = "Homoscedastic (Model SE)")
hist(rec[, 4], freq = F, main = "Heteroscedastic (Model SE)")
hist(rec[, 6], freq = F, main = "Homoscedastic (Sandw SE)")
hist(rec[, 8], freq = F, main = "Heteroscedastic (Sandw SE)")
fileName <- url("https://raw.githubusercontent.com/ysamwang/btry6020_sp22/main/lectureData/estate.csv")
housing_data <- read.csv(fileName)
names(housing_data)
#fitting the model
hmod <- lm(log(price)~log(area) + log(lot) + bed + bath, data = housing_data)
#Testing for heteroscedasticity
bptest(hmod)    # there is heteroscedasticity in the data
coeftest(hmod)   # model based standard error
coeftest(hmod,vcov.=vcovHC(hmod,type = "HC3"))   # robust standard error
sim.size <- 1000
n <- 50
rec1 <- rep(0, sim.size)
for(i in 1:sim.size){
rec1[i] <- mean(rgamma(n, 1, 1))
}
hist(rec1,breaks=20)
x <- rgamma(n, 1, 1)
newX <- sample(x, replace = T)  # draw a sample with the same size as x with replacement
sim.size <- 1000
n <- 50
rec2 <- rep(0, sim.size)
x <- rgamma(n, 1, 1)
for(i in 1:sim.size){
rec2[i] <- mean(sample(x, replace = T) )
}
hist(rec2,breaks=20)
par(mfrow=c(1,2))
hist(rec1,breaks = 20)
hist(rec2,breaks = 20)
rm(list=ls())
knitr::opts_chunk$set(echo = TRUE)
## Homoscedastic linear model
# Fixed Covariates
# Using Wild Bootstrap
sim.size <- 5000
b <- 1
n <- 400
# Fixed Design
x <- seq(0, 10, by = 10/(n-1))
y <- 1 + b * x + rnorm(n)
## We are interested in the estimated coefficient
mod <- lm(y~ x)
observed.stat <- summary(mod)$coef[2, 1]
### Approximate sampling distribution using the Wild and empirical bootstrap
rec.boot <- matrix(0, sim.size, 2)
for(i in 1:sim.size){
# Wild Bootstrap
y.boot.wild <- mod$fitted + mod$residuals * rnorm(n)
# Calculate the statistic for the bootstrap sample
rec.boot[i, 1] <- summary(lm(y.boot.wild ~ x))$coeff[2,1] - observed.stat
# Pairs Bootstrap
ind <- sample(n, replace = T)
x.boot.emp <- x[ind]
y.boot.emp <- y[ind]
# Calculate the statistic for the bootstrap sample
rec.boot[i, 2] <- summary(lm(y.boot.emp ~ x.boot.emp))$coeff[2,1] - observed.stat
}
rec.sim <- rep(0, sim.size)
for(i in 1:sim.size){
## Don't re-draw X
x.sim <- x
y.sim <- 1 + b * x.sim + rnorm(n)
rec.sim[i] <- summary(lm(y.sim ~ x.sim))$coeff[2,1] - b
}
par(mfrow = c(1, 2))
### Bootstrapped sampling distribution in gray histogram
hist(rec.boot[,1], freq = F, col = "gray", breaks = 15, main = "Wild", xlab = expression(hat(b)))
### Simulated Sampling Distribution in red
lines(density(rec.sim), col = "red", lwd = 2)
### Model Based Sampling Distribution in Blue
lines(seq(-5, 5, by = .01), dnorm(seq(-5, 5, by = .01), mean = 0, sd = sqrt(1 / sum((x - mean(x))^2))), col = "blue", lwd = 2)
### Bootstrapped sampling distribution in gray histogram
hist(rec.boot[,2], freq = F, col = "gray", breaks = 15, main = "Pairs", xlab =  expression(hat(b)))
### Simulated Sampling Distribution in red
lines(density(rec.sim), col = "red", lwd = 2)
### Model Based Sampling Distribution in Blue
lines(seq(-5, 5, by = .01), dnorm(seq(-5, 5, by = .01), mean = 0, sd = sqrt(1 / sum((x - mean(x))^2))), col = "blue", lwd = 2)
## Homoscedastic linear model
# Fixed Covariates
# Using Wild Bootstrap
sim.size <- 5000
b <- 1
n <- 2000
# Fixed Design
x <- seq(0, 10, by = 10/(n-1))
y <- 1 + b * x + rnorm(n, sd = x / 3)   # error gets bigger as x gets bigger, heterocedasticity
## We are interested in the estimated coefficient
mod <- lm(y~ x)
observed.stat <- summary(mod)$coef[2, 1]
### Approximate sampling distribution using the Wild and empirical bootstrap
rec.boot <- matrix(0, sim.size, 2)
for(i in 1:sim.size){
# Wild Bootstrap
y.boot.wild <- mod$fitted + mod$residuals * rnorm(n)
# Calculate the statistic for the bootstrap sample
rec.boot[i, 1] <- summary(lm(y.boot.wild ~ x))$coeff[2,1] - observed.stat
# Pairs Bootstrap
ind <- sample(n, replace = T)
x.boot.emp <- x[ind]
y.boot.emp <- y[ind]
# Calculate the statistic for the bootstrap sample
rec.boot[i, 2] <- summary(lm(y.boot.emp ~ x.boot.emp))$coeff[2,1] - observed.stat
}
rec.sim <- rep(0, sim.size)
for(i in 1:sim.size){
# don't redraw X
x.sim <- x
y.sim <- 1 + b * x.sim + rnorm(n, sd = x / 3)
rec.sim[i] <- summary(lm(y.sim ~ x.sim))$coeff[2,1] - b
}
par(mfrow = c(1, 2))
### Bootstrapped sampling distribution in gray histogram
hist(rec.boot[,1], freq = F, col = "gray", breaks = 15, main = "Wild", xlab = expression(hat(b)))
### Simulated Sampling Distribution in red
lines(density(rec.sim), col = "red", lwd = 2)
### Model Based Sampling Distribution in Blue
lines(seq(-5, 5, by = .01), dnorm(seq(-5, 5, by = .01), mean = 0, sd = sqrt(1 / sum((x - mean(x))^2))), col = "blue", lwd = 2)
### Bootstrapped sampling distribution in gray histogram
hist(rec.boot[,2], freq = F, col = "gray", breaks = 15, main = "Pairs", xlab =  expression(hat(b)))
### Simulated Sampling Distribution in red
lines(density(rec.sim), col = "red", lwd = 2)
### Model Based Sampling Distribution in Blue
lines(seq(-5, 5, by = .01), dnorm(seq(-5, 5, by = .01), mean = 0, sd = sqrt(1 / sum((x - mean(x))^2))), col = "blue", lwd = 2)
# install.packages("lmboot")
library("lmboot")
library("lmtest")
library("sandwich")
fileName <- "https://raw.githubusercontent.com/ysamwang/btry6020_sp22/main/lectureData/estate.csv"
housing_data <- read.csv(fileName)
names(housing_data)
mod <- lm(log(price) ~ log(area) + bed + bath + garage + quality,
data = housing_data)
summary(mod)
library(lme4)
fileName <- "https://raw.githubusercontent.com/ysamwang/btry6020_sp22/main/lectureData/grace_plot_level.csv"
dat <- read.csv(fileName)
# Remove Missing Data
# Generally, we want to be careful about the data we remove
# As this may bias our estimates if the missingness is important
dat <- na.omit(dat)
dim(dat)
# Fit a linear model which disregards the site structure
mod <- lm(ln.prich~ ln.ptotmass + ln.pprod, data = dat)
summary(mod)
dat
plot(mod)
rm(list = ls())
knitr::opts_chunk$set(echo = TRUE)
# hint: when feeding in the covariates, you will need to remove the first two columns
# from the bike_data_training set which correspond to the date and bike_counts
# so you will need to feed the function
x = as.matrix(bike_data_train[, - c(1,2)])
bike_data_train <- read.csv("https://raw.githubusercontent.com/ysamwang/btry6020_sp22/main/lectureData/bike_data_18.csv")
dim(bike_data_train)
names(bike_data_train)
# hint: when feeding in the covariates, you will need to remove the first two columns
# from the bike_data_training set which correspond to the date and bike_counts
# so you will need to feed the function
x = as.matrix(bike_data_train[, - c(1,2)])
library(glmnet)
cv_lasso_fit <- cv.glmnet(x = x, y = bike_data_train$bike_counts, alpha = 1, family = "poisson")
plot(cv_lasso_fit)
coef(cv_lasso_fit, s = cv_lasso_fit$lambda.min)
# hint: when feeding in the covariates, you will need to remove the first two columns
# from the bike_data_training set which correspond to the date and bike_counts
# so you will need to feed the function
x = as.matrix(bike_data_train[, - c(1,2)])
library(glmnet)
cv_lasso_fit <- cv.glmnet(x = x, y = bike_data_train$bike_counts, alpha = 1, family = "poisson")
plot(cv_lasso_fit)
coef(cv_lasso_fit, s = cv_lasso_fit$lambda.min)
coef(cv_lasso_fit, s = lasso_mod$lambda.1se)
# hint: when feeding in the covariates, you will need to remove the first two columns
# from the bike_data_training set which correspond to the date and bike_counts
# so you will need to feed the function
x = as.matrix(bike_data_train[, - c(1,2)])
library(glmnet)
cv_lasso_fit <- cv.glmnet(x = x, y = bike_data_train$bike_counts, alpha = 1, family = "poisson")
plot(cv_lasso_fit)
coef(cv_lasso_fit, s = cv_lasso_fit$lambda.min)
coef(cv_lasso_fit, s = lasso_fit$lambda.1se)
# hint: when feeding in the covariates, you will need to remove the first two columns
# from the bike_data_training set which correspond to the date and bike_counts
# so you will need to feed the function
x = as.matrix(bike_data_train[, - c(1,2)])
library(glmnet)
cv_lasso_fit <- cv.glmnet(x = x, y = bike_data_train$bike_counts, alpha = 1, family = "poisson")
plot(cv_lasso_fit)
coef(cv_lasso_fit, s = cv_lasso_fit$lambda.min)
coef(cv_lasso_fit, s = cv_lasso_fit$lambda.1se)
cv_ridge_fit <- cv.glmnet(x = x, y = bike_data_train$bike_counts, alpha = 0, family = "posson")
cv_ridge_fit <- cv.glmnet(x = x, y = bike_data_train$bike_counts, alpha = 0, family = "poisson")
plot(cv_ridge_fit)
coef(cv_ridge_fit, s = cv_ridge_fit$lambda.min)
coef(cv_ridge_fit, s = cv_ridge_fit$lambda.1se)
# Test data from 2019
bike_data_test <- read.csv("https://raw.githubusercontent.com/ysamwang/btry6020_sp22/main/lectureData/bike_data_19.csv")
# regular glm without any penalization
unpenalized_model <- glm(bike_counts ~ ., data = bike_data_train[, -1], family = "poisson")
# predictive accuracy for 2019 when using all covariates but no model seletion or penalization
## use type = "response" to get predictions in bikes, instead of log(bikes)
mean((bike_data_test$bike_counts - predict(unpenalized_model, newx = as.matrix(bike_data_test[,-c(1)]), type = "response"))^2)
## to get predictions for a penalized regression
## use type = "response" to get predictions in bikes, instead of log(bikes)
lasso_predicted <- predict(cv_lasso_fit, s = cv_lasso_fit$lambda.min, newx = x, type = "response")
lasso_test_error <- mean((bike_data_train$bike_counts - lasso_predicted)^2)
lasso_test_error
ridge_predicted <- predict(cv_ridge_fit, s = cv_ridge_fit$lambda.min, newx = x, type = "response")
ridge_test_error <- mean((bike_data_train$bike_counts - ridge_predicted)^2)
ridge_test_error
# hint: when feeding in the covariates, you will need to remove the first two columns
# from the bike_data_training set which correspond to the date and bike_counts
# so you will need to feed the function
x = as.matrix(bike_data_train[, - c(1,2)])
library(glmnet)
cv_lasso_fit <- cv.glmnet(x = x, y = bike_data_train$bike_counts, alpha = 1, family = "poisson")
plot(cv_lasso_fit)
coef(cv_lasso_fit, s = cv_lasso_fit$lambda.min)
coef(cv_lasso_fit, s = cv_lasso_fit$lambda.1se)
cv_ridge_fit <- cv.glmnet(x = x, y = bike_data_train$bike_counts, alpha = 0, family = "poisson")
plot(cv_ridge_fit)
coef(cv_ridge_fit, s = cv_ridge_fit$lambda.min)
coef(cv_ridge_fit, s = cv_ridge_fit$lambda.1se)
# Test data from 2019
bike_data_test <- read.csv("https://raw.githubusercontent.com/ysamwang/btry6020_sp22/main/lectureData/bike_data_19.csv")
# regular glm without any penalization
unpenalized_model <- glm(bike_counts ~ ., data = bike_data_train[, -1], family = "poisson")
# predictive accuracy for 2019 when using all covariates but no model seletion or penalization
## use type = "response" to get predictions in bikes, instead of log(bikes)
mean((bike_data_test$bike_counts - predict(unpenalized_model, newx = as.matrix(bike_data_test[,-c(1)]), type = "response"))^2)
## to get predictions for a penalized regression
## use type = "response" to get predictions in bikes, instead of log(bikes)
lasso_predicted <- predict(cv_lasso_fit, s = cv_lasso_fit$lambda.min, newx = x, type = "response")
lasso_test_error <- mean((bike_data_train$bike_counts - lasso_predicted)^2)
lasso_test_error
ridge_predicted <- predict(cv_ridge_fit, s = cv_ridge_fit$lambda.min, newx = x, type = "response")
ridge_test_error <- mean((bike_data_train$bike_counts - ridge_predicted)^2)
ridge_test_error
rm(list=ls())
library(shiny); runApp('GitHub/Bacillus-cereus-exposure-assessment-model/BRisk app/BRisk.R')
ModelData
runApp('GitHub/Bacillus-cereus-exposure-assessment-model/BRisk app/BRisk.R')
str(df2)
runApp('GitHub/Bacillus-cereus-exposure-assessment-model/BRisk app/BRisk.R')
rm(list=ls())
library(shiny); runApp('GitHub/Bacillus-cereus-exposure-assessment-model/BRisk app/BRisk.R')
rm(list=ls())
rm(list=ls())
library(shiny); runApp('GitHub/Bacillus-cereus-exposure-assessment-model/BRisk app/BRisk.R')
rm(list=ls())
rm(list=ls())
library(shiny); runApp('GitHub/Bacillus-cereus-exposure-assessment-model/BRisk app/BRisk.R')
runApp('GitHub/Bacillus-cereus-exposure-assessment-model/BRisk app/BRisk.R')
rm(list=ls())
m(list=ls())
rm(list=ls())
rm(list=ls())
load("~/GitHub/MC_MilkSpoilageModel_Combined/Combined_model.RData")
# Determine percent spoiled
model_data$t_H = rep(1:end_day, times = n_sim * n_units)
filtered_counts <- map(1:end_day, ~ filter(model_data, t_H == .x) %>% pull(count_H))
percentages <- map(filtered_counts, ~ sum(. > log10(20000)) / length(.))
library(purrr)
filtered_counts <- map(1:end_day, ~ filter(model_data, t_H == .x) %>% pull(count_H))
library(tidyverse)
filtered_counts <- map(1:end_day, ~ filter(model_data, t_H == .x) %>% pull(count_H))
percentages <- map(filtered_counts, ~ sum(. > log10(20000)) / length(.))
percent_spoil <- data.frame(day = 1:end_day, percentage = unlist(percentages))
percent_spoil$percentage = percent_spoil$percentage * 100
percent_spoil
save.image("C:/Users/sujun/Documents/GitHub/MC_MilkSpoilageModel_Combined/Combined_model.RData")
library(shiny); runApp('GitHub/MC_MilkSpoilageModel_Combined/Combined Spoilage Model App.R')
View(model_data)
model_data$newLag_F <- lagAtNewTemp(model_data$T_F, growth_parameters$lag[model_data$allele_index], T0=growth_parameters$T0[model_data$allele_index])
model_data$newMu_F <- muAtNewTemp(model_data$T_F,growth_parameters$mumax[model_data$allele_index],T0=growth_parameters$T0[model_data$allele_index])
# Determine count_F
model_data = model_data %>%
rowwise() %>%
mutate(count_F = log10N(log10_init_mL, log10Nmax, newLag_F, newMu_F, t_F, model_type))
# Stage 2: Transport from facility to retail store
# Determine NewLag_T and NewMu_T
model_data$newLag_T <- lagAtNewTemp(model_data$T_T, growth_parameters$lag[model_data$allele_index], T0=growth_parameters$T0[model_data$allele_index])
model_data$newMu_T <- muAtNewTemp(model_data$T_T,growth_parameters$mumax[model_data$allele_index],T0=growth_parameters$T0[model_data$allele_index])
# Determine count_T
model_data = model_data %>%
rowwise() %>%
mutate(Lag_T = max(0, 1 - t_F/newLag_F) * newLag_T) %>%
mutate(Mu_T = if_else(T_T >= T_F * 0.75 & T_T <= T_F * 1.25, newMu_F, newMu_T)) %>%
mutate(count_T = log10N(count_F, log10Nmax, Lag_T, Mu_T, t_T, model_type))
# Stage 3: Display at retail
# Determine NewLag_S and NewMu_S
model_data$newLag_S <- lagAtNewTemp(model_data$T_S, growth_parameters$lag[model_data$allele_index], T0=growth_parameters$T0[model_data$allele_index])
model_data$newMu_S <- muAtNewTemp(model_data$T_S,growth_parameters$mumax[model_data$allele_index],T0=growth_parameters$T0[model_data$allele_index])
# Determine count_S
model_data = model_data %>%
rowwise() %>%
mutate(Lag_S = max(0, 1 - t_T/Lag_T) * newLag_S) %>%
mutate(Mu_S = if_else(T_S >= T_T * 0.75 & T_S <= T_T * 1.25, newMu_T, newMu_S)) %>%
mutate(count_S = log10N(count_T, log10Nmax, Lag_S, Mu_S, t_S, model_type))
# Stage 4: Transport from retail store to consumer home
# Determine NewLag_T2 and NewMu_T2
model_data$newLag_T2 <- lagAtNewTemp(model_data$T_T2, growth_parameters$lag[model_data$allele_index], T0=growth_parameters$T0[model_data$allele_index])
model_data$newMu_T2 <- muAtNewTemp(model_data$T_T2,growth_parameters$mumax[model_data$allele_index],T0=growth_parameters$T0[model_data$allele_index])
# Determine count_T2
model_data = model_data %>%
rowwise() %>%
mutate(Lag_T2 = max(0, 1 - t_S/Lag_S) * newLag_T2) %>%
mutate(Mu_T2 = if_else(T_T2 >= T_S * 0.75 & T_T2 <= T_S * 1.25, newMu_S, newMu_T2)) %>%
mutate(count_T2 = log10N(count_S, log10Nmax, Lag_T2, Mu_T2, t_T2, model_type))
# Stage 5: Storage at homes
# Determine NewLag_H and NewMu_H
model_data$newLag_H <- lagAtNewTemp(model_data$T_H, growth_parameters$lag[model_data$allele_index], T0=growth_parameters$T0[model_data$allele_index])
model_data$newMu_H <- muAtNewTemp(model_data$T_H,growth_parameters$mumax[model_data$allele_index],T0=growth_parameters$T0[model_data$allele_index])
# Determine count_H
end_day = input$day
model_data = model_data %>%
slice(rep(1:n(), each = end_day))
end_day = 35
model_data = model_data %>%
slice(rep(1:n(), each = end_day))
model_data = model_data %>%
rowwise() %>%
mutate(Lag_H = max(0, 1 - t_T2/Lag_T2) * newLag_H) %>%
mutate(Mu_H = if_else(T_H >= T_T2 * 0.75 & T_H <= T_T2 * 1.25, newMu_T2, newMu_H)) %>%
mutate(count_H = log10N(count_T2, log10Nmax, Lag_H, Mu_H, t_H, model_type))
View(moddel_data)
view(model_data)
rm(list=ls())
library(shiny); runApp('GitHub/MC_MilkSpoilageModel_Combined/Combined Spoilage Model App.R')
runApp('GitHub/MC_MilkSpoilageModel_Combined/Combined Spoilage Model App.R')
rm(list=ls())
library(shiny); runApp('GitHub/Bacillus-cereus-exposure-assessment-model/BRisk app/BRisk.R')
library(shiny); runApp('BRisk.R')
runApp('BRisk.R')
runApp('BRisk.R')
runApp('BRisk.R')
runApp('BRisk.R')
runApp('BRisk.R')
runApp('BRisk.R')
runApp('BRisk.R')
runApp('BRisk.R')
