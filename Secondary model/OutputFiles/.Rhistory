y1 <- 1 + b * x + rnorm(n, sd = sd1)
# generate Y2 with heteroscedasticity
y2 <- 1 + b * x + rnorm(n, sd = x)
# fit linear model
mod1 <- lm(y1 ~ x)
mod2 <- lm(y2 ~ x)
rec[i, ] <- c(  coeftest(mod1, vcov. = vcovHC(mod1, type = "HC3"))[2,3:4],
coeftest(mod2, vcov. = vcovHC(mod2, type = "HC3"))[2,3:4])
}
x <- seq(-5, 5, by = .01)
par(mfrow = c(1,2))
hist(rec[, 1], freq = F, main = "Homoscedastic")
lines(x, dt(x, df = n - 2), col = "red")
hist(rec[, 3], freq = F, main = "Heteroscedastic")
lines(x, dt(x, df = n - 2), col = "red")
par(mfrow = c(1,2))
hist(rec[, 2], freq = F, main = "Homoscedastic")
hist(rec[, 4], freq = F, main = "Heteroscedastic")  # more constant p-values
# When using robust standard errors, losing a little bit power, more likely to make Type II error
sim.size <- 1000
b <- 1/4
n <- 20
# generate covariate X
rec <- matrix(0, sim.size, 8)
for(i in 1:sim.size){
x <- rgamma(n, 1, 1)
# generate Y2 with homoscedasticity
sd1 <- mean(x)
y1 <- 1 + b * x + rnorm(n, sd = sd1)
# generate Y2 with heteroscedasticity
y2 <- 1 + b * x + rnorm(n, sd = x)
# fit linear model
mod1 <- lm(y1 ~ x)
mod2 <- lm(y2 ~ x)
output1 <- summary(mod1)$coeff
output2 <- summary(mod2)$coeff
rec[i, ] <- c(output1[2,3:4],
output2[2,3:4],
coeftest(mod1, vcov. = vcovHC(mod1, type = "HC3"))[2,3:4],
coeftest(mod2, vcov. = vcovHC(mod2, type = "HC3"))[2,3:4])
}
par(mfrow = c(2,2))
hist(rec[, 2], freq = F, main = "Homoscedastic (Model SE)")
hist(rec[, 4], freq = F, main = "Heteroscedastic (Model SE)")
hist(rec[, 6], freq = F, main = "Homoscedastic (Sandw SE)")
hist(rec[, 8], freq = F, main = "Heteroscedastic (Sandw SE)")
fileName <- url("https://raw.githubusercontent.com/ysamwang/btry6020_sp22/main/lectureData/estate.csv")
housing_data <- read.csv(fileName)
names(housing_data)
#fitting the model
hmod <- lm(log(price)~log(area) + log(lot) + bed + bath, data = housing_data)
#Testing for heteroscedasticity
bptest(hmod)    # there is heteroscedasticity in the data
coeftest(hmod)   # model based standard error
coeftest(hmod,vcov.=vcovHC(hmod,type = "HC3"))   # robust standard error
sim.size <- 1000
n <- 50
rec1 <- rep(0, sim.size)
for(i in 1:sim.size){
rec1[i] <- mean(rgamma(n, 1, 1))
}
hist(rec1,breaks=20)
x <- rgamma(n, 1, 1)
newX <- sample(x, replace = T)  # draw a sample with the same size as x with replacement
sim.size <- 1000
n <- 50
rec2 <- rep(0, sim.size)
x <- rgamma(n, 1, 1)
for(i in 1:sim.size){
rec2[i] <- mean(sample(x, replace = T) )
}
hist(rec2,breaks=20)
par(mfrow=c(1,2))
hist(rec1,breaks = 20)
hist(rec2,breaks = 20)
rm(list=ls())
knitr::opts_chunk$set(echo = TRUE)
## Homoscedastic linear model
# Fixed Covariates
# Using Wild Bootstrap
sim.size <- 5000
b <- 1
n <- 400
# Fixed Design
x <- seq(0, 10, by = 10/(n-1))
y <- 1 + b * x + rnorm(n)
## We are interested in the estimated coefficient
mod <- lm(y~ x)
observed.stat <- summary(mod)$coef[2, 1]
### Approximate sampling distribution using the Wild and empirical bootstrap
rec.boot <- matrix(0, sim.size, 2)
for(i in 1:sim.size){
# Wild Bootstrap
y.boot.wild <- mod$fitted + mod$residuals * rnorm(n)
# Calculate the statistic for the bootstrap sample
rec.boot[i, 1] <- summary(lm(y.boot.wild ~ x))$coeff[2,1] - observed.stat
# Pairs Bootstrap
ind <- sample(n, replace = T)
x.boot.emp <- x[ind]
y.boot.emp <- y[ind]
# Calculate the statistic for the bootstrap sample
rec.boot[i, 2] <- summary(lm(y.boot.emp ~ x.boot.emp))$coeff[2,1] - observed.stat
}
rec.sim <- rep(0, sim.size)
for(i in 1:sim.size){
## Don't re-draw X
x.sim <- x
y.sim <- 1 + b * x.sim + rnorm(n)
rec.sim[i] <- summary(lm(y.sim ~ x.sim))$coeff[2,1] - b
}
par(mfrow = c(1, 2))
### Bootstrapped sampling distribution in gray histogram
hist(rec.boot[,1], freq = F, col = "gray", breaks = 15, main = "Wild", xlab = expression(hat(b)))
### Simulated Sampling Distribution in red
lines(density(rec.sim), col = "red", lwd = 2)
### Model Based Sampling Distribution in Blue
lines(seq(-5, 5, by = .01), dnorm(seq(-5, 5, by = .01), mean = 0, sd = sqrt(1 / sum((x - mean(x))^2))), col = "blue", lwd = 2)
### Bootstrapped sampling distribution in gray histogram
hist(rec.boot[,2], freq = F, col = "gray", breaks = 15, main = "Pairs", xlab =  expression(hat(b)))
### Simulated Sampling Distribution in red
lines(density(rec.sim), col = "red", lwd = 2)
### Model Based Sampling Distribution in Blue
lines(seq(-5, 5, by = .01), dnorm(seq(-5, 5, by = .01), mean = 0, sd = sqrt(1 / sum((x - mean(x))^2))), col = "blue", lwd = 2)
## Homoscedastic linear model
# Fixed Covariates
# Using Wild Bootstrap
sim.size <- 5000
b <- 1
n <- 2000
# Fixed Design
x <- seq(0, 10, by = 10/(n-1))
y <- 1 + b * x + rnorm(n, sd = x / 3)   # error gets bigger as x gets bigger, heterocedasticity
## We are interested in the estimated coefficient
mod <- lm(y~ x)
observed.stat <- summary(mod)$coef[2, 1]
### Approximate sampling distribution using the Wild and empirical bootstrap
rec.boot <- matrix(0, sim.size, 2)
for(i in 1:sim.size){
# Wild Bootstrap
y.boot.wild <- mod$fitted + mod$residuals * rnorm(n)
# Calculate the statistic for the bootstrap sample
rec.boot[i, 1] <- summary(lm(y.boot.wild ~ x))$coeff[2,1] - observed.stat
# Pairs Bootstrap
ind <- sample(n, replace = T)
x.boot.emp <- x[ind]
y.boot.emp <- y[ind]
# Calculate the statistic for the bootstrap sample
rec.boot[i, 2] <- summary(lm(y.boot.emp ~ x.boot.emp))$coeff[2,1] - observed.stat
}
rec.sim <- rep(0, sim.size)
for(i in 1:sim.size){
# don't redraw X
x.sim <- x
y.sim <- 1 + b * x.sim + rnorm(n, sd = x / 3)
rec.sim[i] <- summary(lm(y.sim ~ x.sim))$coeff[2,1] - b
}
par(mfrow = c(1, 2))
### Bootstrapped sampling distribution in gray histogram
hist(rec.boot[,1], freq = F, col = "gray", breaks = 15, main = "Wild", xlab = expression(hat(b)))
### Simulated Sampling Distribution in red
lines(density(rec.sim), col = "red", lwd = 2)
### Model Based Sampling Distribution in Blue
lines(seq(-5, 5, by = .01), dnorm(seq(-5, 5, by = .01), mean = 0, sd = sqrt(1 / sum((x - mean(x))^2))), col = "blue", lwd = 2)
### Bootstrapped sampling distribution in gray histogram
hist(rec.boot[,2], freq = F, col = "gray", breaks = 15, main = "Pairs", xlab =  expression(hat(b)))
### Simulated Sampling Distribution in red
lines(density(rec.sim), col = "red", lwd = 2)
### Model Based Sampling Distribution in Blue
lines(seq(-5, 5, by = .01), dnorm(seq(-5, 5, by = .01), mean = 0, sd = sqrt(1 / sum((x - mean(x))^2))), col = "blue", lwd = 2)
# install.packages("lmboot")
library("lmboot")
library("lmtest")
library("sandwich")
fileName <- "https://raw.githubusercontent.com/ysamwang/btry6020_sp22/main/lectureData/estate.csv"
housing_data <- read.csv(fileName)
names(housing_data)
mod <- lm(log(price) ~ log(area) + bed + bath + garage + quality,
data = housing_data)
summary(mod)
library(lme4)
fileName <- "https://raw.githubusercontent.com/ysamwang/btry6020_sp22/main/lectureData/grace_plot_level.csv"
dat <- read.csv(fileName)
# Remove Missing Data
# Generally, we want to be careful about the data we remove
# As this may bias our estimates if the missingness is important
dat <- na.omit(dat)
dim(dat)
# Fit a linear model which disregards the site structure
mod <- lm(ln.prich~ ln.ptotmass + ln.pprod, data = dat)
summary(mod)
dat
plot(mod)
rm(list = ls())
knitr::opts_chunk$set(echo = TRUE)
# hint: when feeding in the covariates, you will need to remove the first two columns
# from the bike_data_training set which correspond to the date and bike_counts
# so you will need to feed the function
x = as.matrix(bike_data_train[, - c(1,2)])
bike_data_train <- read.csv("https://raw.githubusercontent.com/ysamwang/btry6020_sp22/main/lectureData/bike_data_18.csv")
dim(bike_data_train)
names(bike_data_train)
# hint: when feeding in the covariates, you will need to remove the first two columns
# from the bike_data_training set which correspond to the date and bike_counts
# so you will need to feed the function
x = as.matrix(bike_data_train[, - c(1,2)])
library(glmnet)
cv_lasso_fit <- cv.glmnet(x = x, y = bike_data_train$bike_counts, alpha = 1, family = "poisson")
plot(cv_lasso_fit)
coef(cv_lasso_fit, s = cv_lasso_fit$lambda.min)
# hint: when feeding in the covariates, you will need to remove the first two columns
# from the bike_data_training set which correspond to the date and bike_counts
# so you will need to feed the function
x = as.matrix(bike_data_train[, - c(1,2)])
library(glmnet)
cv_lasso_fit <- cv.glmnet(x = x, y = bike_data_train$bike_counts, alpha = 1, family = "poisson")
plot(cv_lasso_fit)
coef(cv_lasso_fit, s = cv_lasso_fit$lambda.min)
coef(cv_lasso_fit, s = lasso_mod$lambda.1se)
# hint: when feeding in the covariates, you will need to remove the first two columns
# from the bike_data_training set which correspond to the date and bike_counts
# so you will need to feed the function
x = as.matrix(bike_data_train[, - c(1,2)])
library(glmnet)
cv_lasso_fit <- cv.glmnet(x = x, y = bike_data_train$bike_counts, alpha = 1, family = "poisson")
plot(cv_lasso_fit)
coef(cv_lasso_fit, s = cv_lasso_fit$lambda.min)
coef(cv_lasso_fit, s = lasso_fit$lambda.1se)
# hint: when feeding in the covariates, you will need to remove the first two columns
# from the bike_data_training set which correspond to the date and bike_counts
# so you will need to feed the function
x = as.matrix(bike_data_train[, - c(1,2)])
library(glmnet)
cv_lasso_fit <- cv.glmnet(x = x, y = bike_data_train$bike_counts, alpha = 1, family = "poisson")
plot(cv_lasso_fit)
coef(cv_lasso_fit, s = cv_lasso_fit$lambda.min)
coef(cv_lasso_fit, s = cv_lasso_fit$lambda.1se)
cv_ridge_fit <- cv.glmnet(x = x, y = bike_data_train$bike_counts, alpha = 0, family = "posson")
cv_ridge_fit <- cv.glmnet(x = x, y = bike_data_train$bike_counts, alpha = 0, family = "poisson")
plot(cv_ridge_fit)
coef(cv_ridge_fit, s = cv_ridge_fit$lambda.min)
coef(cv_ridge_fit, s = cv_ridge_fit$lambda.1se)
# Test data from 2019
bike_data_test <- read.csv("https://raw.githubusercontent.com/ysamwang/btry6020_sp22/main/lectureData/bike_data_19.csv")
# regular glm without any penalization
unpenalized_model <- glm(bike_counts ~ ., data = bike_data_train[, -1], family = "poisson")
# predictive accuracy for 2019 when using all covariates but no model seletion or penalization
## use type = "response" to get predictions in bikes, instead of log(bikes)
mean((bike_data_test$bike_counts - predict(unpenalized_model, newx = as.matrix(bike_data_test[,-c(1)]), type = "response"))^2)
## to get predictions for a penalized regression
## use type = "response" to get predictions in bikes, instead of log(bikes)
lasso_predicted <- predict(cv_lasso_fit, s = cv_lasso_fit$lambda.min, newx = x, type = "response")
lasso_test_error <- mean((bike_data_train$bike_counts - lasso_predicted)^2)
lasso_test_error
ridge_predicted <- predict(cv_ridge_fit, s = cv_ridge_fit$lambda.min, newx = x, type = "response")
ridge_test_error <- mean((bike_data_train$bike_counts - ridge_predicted)^2)
ridge_test_error
# hint: when feeding in the covariates, you will need to remove the first two columns
# from the bike_data_training set which correspond to the date and bike_counts
# so you will need to feed the function
x = as.matrix(bike_data_train[, - c(1,2)])
library(glmnet)
cv_lasso_fit <- cv.glmnet(x = x, y = bike_data_train$bike_counts, alpha = 1, family = "poisson")
plot(cv_lasso_fit)
coef(cv_lasso_fit, s = cv_lasso_fit$lambda.min)
coef(cv_lasso_fit, s = cv_lasso_fit$lambda.1se)
cv_ridge_fit <- cv.glmnet(x = x, y = bike_data_train$bike_counts, alpha = 0, family = "poisson")
plot(cv_ridge_fit)
coef(cv_ridge_fit, s = cv_ridge_fit$lambda.min)
coef(cv_ridge_fit, s = cv_ridge_fit$lambda.1se)
# Test data from 2019
bike_data_test <- read.csv("https://raw.githubusercontent.com/ysamwang/btry6020_sp22/main/lectureData/bike_data_19.csv")
# regular glm without any penalization
unpenalized_model <- glm(bike_counts ~ ., data = bike_data_train[, -1], family = "poisson")
# predictive accuracy for 2019 when using all covariates but no model seletion or penalization
## use type = "response" to get predictions in bikes, instead of log(bikes)
mean((bike_data_test$bike_counts - predict(unpenalized_model, newx = as.matrix(bike_data_test[,-c(1)]), type = "response"))^2)
## to get predictions for a penalized regression
## use type = "response" to get predictions in bikes, instead of log(bikes)
lasso_predicted <- predict(cv_lasso_fit, s = cv_lasso_fit$lambda.min, newx = x, type = "response")
lasso_test_error <- mean((bike_data_train$bike_counts - lasso_predicted)^2)
lasso_test_error
ridge_predicted <- predict(cv_ridge_fit, s = cv_ridge_fit$lambda.min, newx = x, type = "response")
ridge_test_error <- mean((bike_data_train$bike_counts - ridge_predicted)^2)
ridge_test_error
rm(list=ls())
library(shiny); runApp('GitHub/Bacillus-cereus-exposure-assessment-model/BRisk app/BRisk.R')
ModelData
runApp('GitHub/Bacillus-cereus-exposure-assessment-model/BRisk app/BRisk.R')
str(df2)
runApp('GitHub/Bacillus-cereus-exposure-assessment-model/BRisk app/BRisk.R')
rm(list=ls())
library(shiny); runApp('GitHub/Bacillus-cereus-exposure-assessment-model/BRisk app/BRisk.R')
rm(list=ls())
rm(list=ls())
library(shiny); runApp('GitHub/Bacillus-cereus-exposure-assessment-model/BRisk app/BRisk.R')
rm(list=ls())
rm(list=ls())
library(shiny); runApp('GitHub/Bacillus-cereus-exposure-assessment-model/BRisk app/BRisk.R')
runApp('GitHub/Bacillus-cereus-exposure-assessment-model/BRisk app/BRisk.R')
rm(list=ls())
m(list=ls())
rm(list=ls())
rm(list=ls())
load("~/GitHub/MC_MilkSpoilageModel_Combined/Combined_model.RData")
# Determine percent spoiled
model_data$t_H = rep(1:end_day, times = n_sim * n_units)
filtered_counts <- map(1:end_day, ~ filter(model_data, t_H == .x) %>% pull(count_H))
percentages <- map(filtered_counts, ~ sum(. > log10(20000)) / length(.))
library(purrr)
filtered_counts <- map(1:end_day, ~ filter(model_data, t_H == .x) %>% pull(count_H))
library(tidyverse)
filtered_counts <- map(1:end_day, ~ filter(model_data, t_H == .x) %>% pull(count_H))
percentages <- map(filtered_counts, ~ sum(. > log10(20000)) / length(.))
percent_spoil <- data.frame(day = 1:end_day, percentage = unlist(percentages))
percent_spoil$percentage = percent_spoil$percentage * 100
percent_spoil
save.image("C:/Users/sujun/Documents/GitHub/MC_MilkSpoilageModel_Combined/Combined_model.RData")
library(shiny); runApp('GitHub/MC_MilkSpoilageModel_Combined/Combined Spoilage Model App.R')
View(model_data)
model_data$newLag_F <- lagAtNewTemp(model_data$T_F, growth_parameters$lag[model_data$allele_index], T0=growth_parameters$T0[model_data$allele_index])
model_data$newMu_F <- muAtNewTemp(model_data$T_F,growth_parameters$mumax[model_data$allele_index],T0=growth_parameters$T0[model_data$allele_index])
# Determine count_F
model_data = model_data %>%
rowwise() %>%
mutate(count_F = log10N(log10_init_mL, log10Nmax, newLag_F, newMu_F, t_F, model_type))
# Stage 2: Transport from facility to retail store
# Determine NewLag_T and NewMu_T
model_data$newLag_T <- lagAtNewTemp(model_data$T_T, growth_parameters$lag[model_data$allele_index], T0=growth_parameters$T0[model_data$allele_index])
model_data$newMu_T <- muAtNewTemp(model_data$T_T,growth_parameters$mumax[model_data$allele_index],T0=growth_parameters$T0[model_data$allele_index])
# Determine count_T
model_data = model_data %>%
rowwise() %>%
mutate(Lag_T = max(0, 1 - t_F/newLag_F) * newLag_T) %>%
mutate(Mu_T = if_else(T_T >= T_F * 0.75 & T_T <= T_F * 1.25, newMu_F, newMu_T)) %>%
mutate(count_T = log10N(count_F, log10Nmax, Lag_T, Mu_T, t_T, model_type))
# Stage 3: Display at retail
# Determine NewLag_S and NewMu_S
model_data$newLag_S <- lagAtNewTemp(model_data$T_S, growth_parameters$lag[model_data$allele_index], T0=growth_parameters$T0[model_data$allele_index])
model_data$newMu_S <- muAtNewTemp(model_data$T_S,growth_parameters$mumax[model_data$allele_index],T0=growth_parameters$T0[model_data$allele_index])
# Determine count_S
model_data = model_data %>%
rowwise() %>%
mutate(Lag_S = max(0, 1 - t_T/Lag_T) * newLag_S) %>%
mutate(Mu_S = if_else(T_S >= T_T * 0.75 & T_S <= T_T * 1.25, newMu_T, newMu_S)) %>%
mutate(count_S = log10N(count_T, log10Nmax, Lag_S, Mu_S, t_S, model_type))
# Stage 4: Transport from retail store to consumer home
# Determine NewLag_T2 and NewMu_T2
model_data$newLag_T2 <- lagAtNewTemp(model_data$T_T2, growth_parameters$lag[model_data$allele_index], T0=growth_parameters$T0[model_data$allele_index])
model_data$newMu_T2 <- muAtNewTemp(model_data$T_T2,growth_parameters$mumax[model_data$allele_index],T0=growth_parameters$T0[model_data$allele_index])
# Determine count_T2
model_data = model_data %>%
rowwise() %>%
mutate(Lag_T2 = max(0, 1 - t_S/Lag_S) * newLag_T2) %>%
mutate(Mu_T2 = if_else(T_T2 >= T_S * 0.75 & T_T2 <= T_S * 1.25, newMu_S, newMu_T2)) %>%
mutate(count_T2 = log10N(count_S, log10Nmax, Lag_T2, Mu_T2, t_T2, model_type))
# Stage 5: Storage at homes
# Determine NewLag_H and NewMu_H
model_data$newLag_H <- lagAtNewTemp(model_data$T_H, growth_parameters$lag[model_data$allele_index], T0=growth_parameters$T0[model_data$allele_index])
model_data$newMu_H <- muAtNewTemp(model_data$T_H,growth_parameters$mumax[model_data$allele_index],T0=growth_parameters$T0[model_data$allele_index])
# Determine count_H
end_day = input$day
model_data = model_data %>%
slice(rep(1:n(), each = end_day))
end_day = 35
model_data = model_data %>%
slice(rep(1:n(), each = end_day))
model_data = model_data %>%
rowwise() %>%
mutate(Lag_H = max(0, 1 - t_T2/Lag_T2) * newLag_H) %>%
mutate(Mu_H = if_else(T_H >= T_T2 * 0.75 & T_H <= T_T2 * 1.25, newMu_T2, newMu_H)) %>%
mutate(count_H = log10N(count_T2, log10Nmax, Lag_H, Mu_H, t_H, model_type))
View(moddel_data)
view(model_data)
rm(list=ls())
library(shiny); runApp('GitHub/MC_MilkSpoilageModel_Combined/Combined Spoilage Model App.R')
runApp('GitHub/MC_MilkSpoilageModel_Combined/Combined Spoilage Model App.R')
rm(list=ls())
setwd("C:/Users/sujun/Documents/GitHub/Bacillus-cereus-exposure-assessment-model/Primary model/OutputFiles")
gp_22dC = read.csv("gp_22dC_new.csv")
Phylogenetic_Group = c("I","I","II","VII","IV","IV","IV","IV","II","III","IV","II","VII","II","V","V","IV",
"I","I","II","VII","IV","IV","IV","IV","II","III","IV","II","VII","II","V","V","IV")
gp_22dC
Table1 = data.frame(Isolate_name = gp_22dC$isolate, Phylogenetic_group = Phylogenetic_Group,
Rep = gp_22dC$rep,lag = gp_22dC$lag,mumax = gp_22dC$mumax,Nmax = gp_22dC$LOG10Nmax)
Table1
library(dplyr)
Table1 <- arrange(Table1, Phylogenetic_group)
Table1
Table1 <- arrange(Table1, Isolate_name, Rep)
Table1
Table1 <- arrange(Table1, Phylogenetic_group)
Table1
Table1 <- Table1 %>%
group_by(Isolate_name) %>%
arrange(desc(Rep))
Table1
View(Table1)
Table1 <- arrange(Table1, Phylogenetic_group)
Table1
gp_22dC = read.csv("gp_22dC_new.csv")
Phylogenetic_Group = c("I","I","II","VII","IV","IV","IV","IV","II","III","IV","II","VII","II","V","V","IV",
"I","I","II","VII","IV","IV","IV","IV","II","III","IV","II","VII","II","V","V","IV")
Table1 = data.frame(Isolate_name = gp_22dC$isolate, Phylogenetic_group = Phylogenetic_Group,
Rep = gp_22dC$rep,lag = gp_22dC$lag,mumax = gp_22dC$mumax,Nmax = gp_22dC$LOG10Nmax)
Table1 <- arrange(Table1, Phylogenetic_group)
Table1
Table1$Isolate_name <- paste0("PS00", Table1$Isolate_name)
Table1
gp_22dC = read.csv("gp_22dC_new.csv")
rm(list=ls())
gp_22dC = read.csv("gp_22dC_new.csv")
Phylogenetic_Group = c("I","I","II","VII","IV","IV","IV","IV","II","III","IV","II","VII","II","V","V","IV",
"I","I","II","VII","IV","IV","IV","IV","II","III","IV","II","VII","II","V","V","IV")
Table1 = data.frame(Isolate_name = gp_22dC$isolate, Phylogenetic_group = Phylogenetic_Group,
Rep = gp_22dC$rep,lag = gp_22dC$lag,mumax = gp_22dC$mumax,Nmax = gp_22dC$LOG10Nmax)
Table1$Isolate_name <- paste0("PS00", Table1$Isolate_name)
Table1 <- Table1 %>%
arrange(desc(Phylogenetic_group)) %>%
group_by(Isolate_name, Rep) %>%
ungroup()
Table1
View(Table1)
gp_22dC = read.csv("gp_22dC_new.csv")
Phylogenetic_Group = c("I","I","II","VII","IV","IV","IV","IV","II","III","IV","II","VII","II","V","V","IV",
"I","I","II","VII","IV","IV","IV","IV","II","III","IV","II","VII","II","V","V","IV")
Table1 = data.frame(Isolate_name = gp_22dC$isolate, Phylogenetic_group = Phylogenetic_Group,
Rep = gp_22dC$rep,lag = gp_22dC$lag,mumax = gp_22dC$mumax,Nmax = gp_22dC$LOG10Nmax)
Table1$Isolate_name <- paste0("PS00", Table1$Isolate_name)
Table1 <- arrange(Table1, Phylogenetic_group)
Table1
write.csv(Table1,"Table1.csv")
gp_16dC = read.csv("gp_16dC_new.csv")
gp_16dC = read.csv("gp_16dC_new.csv")
Phylogenetic_Group = c("I","I","I","I")
Table2 = data.frame(Isolate_name = gp_16dC$isolate, Phylogenetic_group = Phylogenetic_Group,
Rep = gp_16dC$rep,lag = gp_16dC$lag,mumax = gp_16dC$mumax,Nmax = gp_16dC$LOG10Nmax)
Table2$Isolate_name <- paste0("PS00", Table2$Isolate_name)
write.csv(Table2,"Table2.csv")
Table2
gp_10dC = read.csv("gp_10dC_new.csv")
Phylogenetic_Group = c("II","VII","IV","IV","IV","IV","II","III","IV","II","VII","II","V","V","IV",
"II","VII","IV","IV","IV","IV","II","III","IV","II","VII","II","V","V","IV")
Table3 = data.frame(Isolate_name = gp_10dC$isolate, Phylogenetic_group = Phylogenetic_Group,
Rep = gp_10dC$rep,lag = gp_10dC$lag,mumax = gp_10dC$mumax,Nmax = gp_10dC$LOG10Nmax)
Table3$Isolate_name <- paste0("PS00", Table3$Isolate_name)
Table3 <- arrange(Table3, Phylogenetic_group)
write.csv(Table3,"Table3.csv")
Table3
gp_22dC = read.csv("gp_22dC_new.csv")
Phylogenetic_Group = c("I","I","II","VII","IV","IV","IV","IV","II","III","IV","II","VII","II","V","V","IV",
"I","I","II","VII","IV","IV","IV","IV","II","III","IV","II","VII","II","V","V","IV")
Table1 = data.frame(Isolate_name = gp_22dC$isolate, Phylogenetic_group = Phylogenetic_Group,
Rep = gp_22dC$rep,lag = gp_22dC$lag,mumax = gp_22dC$mumax,Nmax = gp_22dC$LOG10Nmax)
Table1$Isolate_name <- paste0("PS00", Table1$Isolate_name)
Table1 <- arrange(Table1, Phylogenetic_group)
Table1$lag <- round(Table1$lag, 2)
Table1$mumax <- round(Table1$mumax, 2)
Table1$Nmax <- round(Table1$Nmax, 2)
write.csv(Table1,"Table1.csv")
gp_16dC = read.csv("gp_16dC_new.csv")
Phylogenetic_Group = c("I","I","I","I")
Table2 = data.frame(Isolate_name = gp_16dC$isolate, Phylogenetic_group = Phylogenetic_Group,
Rep = gp_16dC$rep,lag = gp_16dC$lag,mumax = gp_16dC$mumax,Nmax = gp_16dC$LOG10Nmax)
Table2$Isolate_name <- paste0("PS00", Table2$Isolate_name)
Table2$lag <- round(Table2$lag, 2)
Table2$mumax <- round(Table2$mumax, 2)
Table2$Nmax <- round(Table2$Nmax, 2)
write.csv(Table2,"Table2.csv")
gp_10dC = read.csv("gp_10dC_new.csv")
Phylogenetic_Group = c("II","VII","IV","IV","IV","IV","II","III","IV","II","VII","II","V","V","IV",
"II","VII","IV","IV","IV","IV","II","III","IV","II","VII","II","V","V","IV")
Table3 = data.frame(Isolate_name = gp_10dC$isolate, Phylogenetic_group = Phylogenetic_Group,
Rep = gp_10dC$rep,lag = gp_10dC$lag,mumax = gp_10dC$mumax,Nmax = gp_10dC$LOG10Nmax)
Table3$Isolate_name <- paste0("PS00", Table3$Isolate_name)
Table3 <- arrange(Table3, Phylogenetic_group)
Table3$lag <- round(Table3$lag, 2)
Table3$mumax <- round(Table3$mumax, 2)
Table3$Nmax <- round(Table3$Nmax, 2)
write.csv(Table3,"Table3.csv")
rm(list=ls())
# 10dC data
data2 <- read.csv("Primary model/InputFiles/data_10_new.csv")
setwd("C:/Users/sujun/Documents/GitHub/Bacillus-cereus-exposure-assessment-model/Primary model/InputFiles")
# 10dC data
data2 <- read.csv("Primary model/InputFiles/data_10_new.csv")
setwd("C:/Users/sujun/Documents/GitHub/Bacillus-cereus-exposure-assessment-model/Primary model/InputFiles")
setwd("C:/Users/sujun/Documents/GitHub/Bacillus-cereus-exposure-assessment-model/Primary model")
# 10dC data
data2 <- read.csv("Primary model/InputFiles/data_10_new.csv")
setwd("C:/Users/sujun/Documents/GitHub/Bacillus-cereus-exposure-assessment-model")
# 10dC data
data2 <- read.csv("Primary model/InputFiles/data_10_new.csv")
# Iso413Rep1 @ 10dC
Iso413Rep1 <- subset(data2, Isolate == "413" & Rep =="rep1")
Iso413Rep1
plot(Iso413Rep1$ï..t,Iso413Rep1$LOG10N)
plot(Iso413Rep1$ï..t,Iso413Rep1$LOG10N,)
plot(Iso413Rep1$ï..t,Iso413Rep1$LOG10N,
main = "Growth data of PS00413Rep1 at 10 degrees",
xlab = "time (h)",
ylab = "count (log10 CFU/mL)")
rm(list=ls())
data = read.csv("mumax_new.csv")
setwd("C:/Users/sujun/Documents/GitHub/Bacillus-cereus-exposure-assessment-model/Secondary model/OutputFiles")
data = read.csv("mumax_new.csv")
data
Iso135 = subset(data,Isolate == 135)
Iso135
plot(Iso135$temp,Iso135$sqrt_mumax_log10_day,
main = "Secondary model of PS00135",
xlab = "Temperature (degrees Celsius)",
ylab = "Square root of mumax (log10 CFU/mL/day)")
